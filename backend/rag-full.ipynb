{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-10-03T19:29:38.547762Z",
     "iopub.status.busy": "2024-10-03T19:29:38.547277Z",
     "iopub.status.idle": "2024-10-03T19:30:03.307990Z",
     "shell.execute_reply": "2024-10-03T19:30:03.306489Z",
     "shell.execute_reply.started": "2024-10-03T19:29:38.547712Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pinecone-client\n",
      "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /opt/conda/lib/python3.10/site-packages (from pinecone-client) (2024.8.30)\n",
      "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n",
      "  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
      "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /opt/conda/lib/python3.10/site-packages (from pinecone-client) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.10/site-packages (from pinecone-client) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from pinecone-client) (1.26.18)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Collecting langchain-core<0.4.0,>=0.3.8 (from langchain)\n",
      "  Downloading langchain_core-0.3.8-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.130-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.3.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.6.7)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.8->langchain) (1.33)\n",
      "Collecting packaging<25,>=23.2 (from langchain-core<0.4.0,>=0.3.8->langchain)\n",
      "  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.4)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.8->langchain) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.0)\n",
      "Downloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading langchain-0.3.2-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.3.1-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.8-py3-none-any.whl (400 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.130-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Downloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
      "Downloading packaging-24.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pinecone-plugin-interface, packaging, requests-toolbelt, pinecone-plugin-inference, pydantic-settings, pinecone-client, langsmith, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.3\n",
      "    Uninstalling packaging-21.3:\n",
      "      Successfully uninstalled packaging-21.3\n",
      "  Attempting uninstall: requests-toolbelt\n",
      "    Found existing installation: requests-toolbelt 0.10.1\n",
      "    Uninstalling requests-toolbelt-0.10.1:\n",
      "      Successfully uninstalled requests-toolbelt-0.10.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\n",
      "jupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\n",
      "kfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "ydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-0.3.2 langchain-community-0.3.1 langchain-core-0.3.8 langchain-text-splitters-0.3.0 langsmith-0.1.130 packaging-24.1 pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0 pinecone-plugin-interface-0.0.7 pydantic-settings-2.5.2 requests-toolbelt-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pinecone langchain langchain-community "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-10-03T19:30:43.882811Z",
     "iopub.status.busy": "2024-10-03T19:30:43.882308Z",
     "iopub.status.idle": "2024-10-03T19:30:59.581201Z",
     "shell.execute_reply": "2024-10-03T19:30:59.579801Z",
     "shell.execute_reply.started": "2024-10-03T19:30:43.882763Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_huggingface\n",
      "  Downloading langchain_huggingface-0.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langchain_huggingface) (0.25.0)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from langchain_huggingface) (0.3.8)\n",
      "Collecting sentence-transformers>=2.6.0 (from langchain_huggingface)\n",
      "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from langchain_huggingface) (0.19.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in /opt/conda/lib/python3.10/site-packages (from langchain_huggingface) (4.44.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.15.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3.0->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3.0->langchain_huggingface) (0.1.130)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3.0->langchain_huggingface) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3.0->langchain_huggingface) (8.3.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.4.0+cpu)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.2.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.14.1)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (10.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.0->langchain_huggingface) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.0->langchain_huggingface) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain_huggingface) (2.4)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (3.10.4)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain_huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain_huggingface) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.8.30)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_huggingface) (1.2.0)\n",
      "Downloading langchain_huggingface-0.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentence-transformers, langchain_huggingface\n",
      "Successfully installed langchain_huggingface-0.1.0 sentence-transformers-3.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T19:30:59.584389Z",
     "iopub.status.busy": "2024-10-03T19:30:59.583855Z",
     "iopub.status.idle": "2024-10-03T19:31:02.831225Z",
     "shell.execute_reply": "2024-10-03T19:31:02.829852Z",
     "shell.execute_reply.started": "2024-10-03T19:30:59.584331Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.docstore.document import Document\n",
    "import langchain_huggingface\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting building docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:46:41.960625Z",
     "iopub.status.busy": "2024-10-03T20:46:41.960160Z",
     "iopub.status.idle": "2024-10-03T20:46:41.967892Z",
     "shell.execute_reply": "2024-10-03T20:46:41.966569Z",
     "shell.execute_reply.started": "2024-10-03T20:46:41.960566Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Neural networks, a subset of machine learning, are computational models inspired by the human brain's neural structure. They consist of layers of nodes (neurons) connected by weighted edges, where each node processes input data through an activation function and passes it to the next layer. These networks are primarily used for tasks like classification, regression, and pattern recognition. The most basic form, the feedforward neural network, processes data in one direction from input to output layers. More advanced architectures include convolutional neural networks (CNNs), which excel at image processing by learning spatial hierarchies, and recurrent neural networks (RNNs), designed for sequence data such as time series or natural language. Training a neural network involves adjusting the weights using optimization algorithms like gradient descent, where backpropagation calculates the error gradient to refine these weights, improving the network's predictions. Neural networks, especially deep learning models, require vast amounts of data and computational power, but once trained, they can perform highly complex tasks, from object recognition to language translation, with remarkable accuracy. Their ability to automatically learn features from raw data has made them a cornerstone of modern AI, transforming industries ranging from healthcare to finance by enabling systems to make decisions and predictions based on previously unattainable levels of insight.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:46:42.251922Z",
     "iopub.status.busy": "2024-10-03T20:46:42.250918Z",
     "iopub.status.idle": "2024-10-03T20:46:42.262314Z",
     "shell.execute_reply": "2024-10-03T20:46:42.260970Z",
     "shell.execute_reply.started": "2024-10-03T20:46:42.251869Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Function to create chunks of sentences with overlap\n",
    "def create_sentence_chunks(sentences, chunk_size, chunk_overlap):\n",
    "    chunks = []\n",
    "    \n",
    "    # Create chunks with overlap\n",
    "    for i in range(0, len(sentences), chunk_size - chunk_overlap):\n",
    "        chunk = ' '.join(sentences[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "        # Stop if we reach the end\n",
    "        if i + chunk_size >= len(sentences):\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "chunk_size = 3  \n",
    "chunk_overlap = 1  \n",
    "\n",
    "# chunks of sentences\n",
    "sentence_chunks = create_sentence_chunks(sentences, chunk_size, chunk_overlap)\n",
    "\n",
    "documents = [Document(page_content=chunk) for chunk in sentence_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:46:42.746261Z",
     "iopub.status.busy": "2024-10-03T20:46:42.745797Z",
     "iopub.status.idle": "2024-10-03T20:46:42.754105Z",
     "shell.execute_reply": "2024-10-03T20:46:42.752500Z",
     "shell.execute_reply.started": "2024-10-03T20:46:42.746216Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "Neural networks, a subset of machine learning, are computational models inspired by the human brain's neural structure. They consist of layers of nodes (neurons) connected by weighted edges, where each node processes input data through an activation function and passes it to the next layer. These networks are primarily used for tasks like classification, regression, and pattern recognition.\n",
      "\n",
      "Document 2:\n",
      "These networks are primarily used for tasks like classification, regression, and pattern recognition. The most basic form, the feedforward neural network, processes data in one direction from input to output layers. More advanced architectures include convolutional neural networks (CNNs), which excel at image processing by learning spatial hierarchies, and recurrent neural networks (RNNs), designed for sequence data such as time series or natural language.\n",
      "\n",
      "Document 3:\n",
      "More advanced architectures include convolutional neural networks (CNNs), which excel at image processing by learning spatial hierarchies, and recurrent neural networks (RNNs), designed for sequence data such as time series or natural language. Training a neural network involves adjusting the weights using optimization algorithms like gradient descent, where backpropagation calculates the error gradient to refine these weights, improving the network's predictions. Neural networks, especially deep learning models, require vast amounts of data and computational power, but once trained, they can perform highly complex tasks, from object recognition to language translation, with remarkable accuracy.\n",
      "\n",
      "Document 4:\n",
      "Neural networks, especially deep learning models, require vast amounts of data and computational power, but once trained, they can perform highly complex tasks, from object recognition to language translation, with remarkable accuracy. Their ability to automatically learn features from raw data has made them a cornerstone of modern AI, transforming industries ranging from healthcare to finance by enabling systems to make decisions and predictions based on previously unattainable levels of insight.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i+1}:\\n{doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:46:44.181982Z",
     "iopub.status.busy": "2024-10-03T20:46:44.181513Z",
     "iopub.status.idle": "2024-10-03T20:46:44.987436Z",
     "shell.execute_reply": "2024-10-03T20:46:44.986148Z",
     "shell.execute_reply.started": "2024-10-03T20:46:44.181938Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:46:44.990541Z",
     "iopub.status.busy": "2024-10-03T20:46:44.990023Z",
     "iopub.status.idle": "2024-10-03T20:46:44.999386Z",
     "shell.execute_reply": "2024-10-03T20:46:44.998108Z",
     "shell.execute_reply.started": "2024-10-03T20:46:44.990482Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:46:47.204836Z",
     "iopub.status.busy": "2024-10-03T20:46:47.204305Z",
     "iopub.status.idle": "2024-10-03T20:46:47.214563Z",
     "shell.execute_reply": "2024-10-03T20:46:47.212672Z",
     "shell.execute_reply.started": "2024-10-03T20:46:47.204778Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pad_embedding(embedding, target_dim=512):\n",
    "    # Convert to numpy array if it's not already\n",
    "    if not isinstance(embedding, np.ndarray):\n",
    "        embedding = np.array(embedding)\n",
    "    \n",
    "    # Calculate the padding needed\n",
    "    current_dim = embedding.shape[-1]\n",
    "    if current_dim < target_dim:\n",
    "        padding = np.zeros(target_dim - current_dim)\n",
    "        padded_embedding = np.concatenate([embedding, padding])\n",
    "    else:\n",
    "        padded_embedding = embedding\n",
    "    \n",
    "    return padded_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:46:47.470655Z",
     "iopub.status.busy": "2024-10-03T20:46:47.470186Z",
     "iopub.status.idle": "2024-10-03T20:46:47.480692Z",
     "shell.execute_reply": "2024-10-03T20:46:47.479141Z",
     "shell.execute_reply.started": "2024-10-03T20:46:47.470610Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_embeddings(df):\n",
    "    all_embeddings = []\n",
    "    for text in df:\n",
    "        # Extract the embedding\n",
    "        embedding = embeddings.embed_query(text)\n",
    "        # Pad the embedding to the desired length (512)\n",
    "        padded_embedding = pad_embedding(embedding, target_dim=512)\n",
    "        all_embeddings.append(padded_embedding)\n",
    "    \n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:46:48.040366Z",
     "iopub.status.busy": "2024-10-03T20:46:48.039509Z",
     "iopub.status.idle": "2024-10-03T20:46:48.262547Z",
     "shell.execute_reply": "2024-10-03T20:46:48.261111Z",
     "shell.execute_reply.started": "2024-10-03T20:46:48.040315Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "document_contents = [doc.page_content for doc in documents]\n",
    "x = extract_embeddings(document_contents) # X contains embeddings for all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:46:49.313609Z",
     "iopub.status.busy": "2024-10-03T20:46:49.313149Z",
     "iopub.status.idle": "2024-10-03T20:46:49.324140Z",
     "shell.execute_reply": "2024-10-03T20:46:49.322727Z",
     "shell.execute_reply.started": "2024-10-03T20:46:49.313543Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pc_api_key = \"62fd34a9-fb37-492c-a395-f2e1f39f350f\"\n",
    "pc = Pinecone(api_key=pc_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:23:21.696489Z",
     "iopub.status.busy": "2024-10-03T20:23:21.696010Z",
     "iopub.status.idle": "2024-10-03T20:23:27.127481Z",
     "shell.execute_reply": "2024-10-03T20:23:27.126034Z",
     "shell.execute_reply.started": "2024-10-03T20:23:21.696441Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pc.create_index(\n",
    "    name=\"fyp-txt-embedding2\",\n",
    "    dimension=384,  \n",
    "    metric=\"dotproduct\", \n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:46:51.973998Z",
     "iopub.status.busy": "2024-10-03T20:46:51.973502Z",
     "iopub.status.idle": "2024-10-03T20:46:51.982808Z",
     "shell.execute_reply": "2024-10-03T20:46:51.981747Z",
     "shell.execute_reply.started": "2024-10-03T20:46:51.973951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "index = pc.Index(name=\"fyp-txt-embedding2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check this cell below, needed or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:46:55.058912Z",
     "iopub.status.busy": "2024-10-03T20:46:55.058460Z",
     "iopub.status.idle": "2024-10-03T20:46:55.217599Z",
     "shell.execute_reply": "2024-10-03T20:46:55.216528Z",
     "shell.execute_reply.started": "2024-10-03T20:46:55.058863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get embeddings for each document\n",
    "document_contents = [doc.page_content for doc in documents]\n",
    "embeddings_list = [embeddings.embed_query(text) for text in document_contents]\n",
    "\n",
    "# Prepare data for upsert: List of tuples (id, vector, metadata)\n",
    "values = [\n",
    "    (str(i), emb, {\"text\": document_contents[i]})  # Convert embedding to list format\n",
    "    for i, emb in enumerate(embeddings_list)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:46:55.459758Z",
     "iopub.status.busy": "2024-10-03T20:46:55.458639Z",
     "iopub.status.idle": "2024-10-03T20:46:56.076160Z",
     "shell.execute_reply": "2024-10-03T20:46:56.074676Z",
     "shell.execute_reply.started": "2024-10-03T20:46:55.459707Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 4}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.upsert(vectors=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:46:57.023493Z",
     "iopub.status.busy": "2024-10-03T20:46:57.022991Z",
     "iopub.status.idle": "2024-10-03T20:46:57.105456Z",
     "shell.execute_reply": "2024-10-03T20:46:57.104276Z",
     "shell.execute_reply.started": "2024-10-03T20:46:57.023446Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 4}},\n",
       " 'total_vector_count': 4}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying from the VDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:46:59.133399Z",
     "iopub.status.busy": "2024-10-03T20:46:59.132938Z",
     "iopub.status.idle": "2024-10-03T20:46:59.245881Z",
     "shell.execute_reply": "2024-10-03T20:46:59.244393Z",
     "shell.execute_reply.started": "2024-10-03T20:46:59.133356Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 0\n",
      "Score: 0.753587306\n",
      "Metadata: {'text': \"Neural networks, a subset of machine learning, are computational models inspired by the human brain's neural structure. They consist of layers of nodes (neurons) connected by weighted edges, where each node processes input data through an activation function and passes it to the next layer. These networks are primarily used for tasks like classification, regression, and pattern recognition.\"}\n",
      "\n",
      "ID: 1\n",
      "Score: 0.749176741\n",
      "Metadata: {'text': 'These networks are primarily used for tasks like classification, regression, and pattern recognition. The most basic form, the feedforward neural network, processes data in one direction from input to output layers. More advanced architectures include convolutional neural networks (CNNs), which excel at image processing by learning spatial hierarchies, and recurrent neural networks (RNNs), designed for sequence data such as time series or natural language.'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define your query text\n",
    "query_text = \"What are neural networks primarily used for?\"\n",
    "\n",
    "# Generate the embedding for the query\n",
    "query_embedding = embeddings.embed_query(query_text)\n",
    "\n",
    "# Perform the query on the Pinecone index\n",
    "response = index.query(\n",
    "    vector=query_embedding,\n",
    "    top_k=2,  # Number of closest vectors to return\n",
    "    include_values=True,  # Include the vectors themselves\n",
    "    include_metadata=True  # Include the metadata (like original text)\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "for match in response[\"matches\"]:\n",
    "    print(f\"ID: {match['id']}\")\n",
    "    print(f\"Score: {match['score']}\")\n",
    "    print(f\"Metadata: {match['metadata']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:47:03.215344Z",
     "iopub.status.busy": "2024-10-03T20:47:03.214894Z",
     "iopub.status.idle": "2024-10-03T20:47:03.221106Z",
     "shell.execute_reply": "2024-10-03T20:47:03.219678Z",
     "shell.execute_reply.started": "2024-10-03T20:47:03.215301Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x = response[\"matches\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:47:03.875522Z",
     "iopub.status.busy": "2024-10-03T20:47:03.875061Z",
     "iopub.status.idle": "2024-10-03T20:47:03.881737Z",
     "shell.execute_reply": "2024-10-03T20:47:03.880366Z",
     "shell.execute_reply.started": "2024-10-03T20:47:03.875475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "z = ''\n",
    "for match in response[\"matches\"]:\n",
    "    z = z + match['metadata']['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:47:04.427721Z",
     "iopub.status.busy": "2024-10-03T20:47:04.427216Z",
     "iopub.status.idle": "2024-10-03T20:47:04.435609Z",
     "shell.execute_reply": "2024-10-03T20:47:04.434303Z",
     "shell.execute_reply.started": "2024-10-03T20:47:04.427672Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Neural networks, a subset of machine learning, are computational models inspired by the human brain's neural structure. They consist of layers of nodes (neurons) connected by weighted edges, where each node processes input data through an activation function and passes it to the next layer. These networks are primarily used for tasks like classification, regression, and pattern recognition.These networks are primarily used for tasks like classification, regression, and pattern recognition. The most basic form, the feedforward neural network, processes data in one direction from input to output layers. More advanced architectures include convolutional neural networks (CNNs), which excel at image processing by learning spatial hierarchies, and recurrent neural networks (RNNs), designed for sequence data such as time series or natural language.\""
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:47:04.872710Z",
     "iopub.status.busy": "2024-10-03T20:47:04.872255Z",
     "iopub.status.idle": "2024-10-03T20:47:04.878974Z",
     "shell.execute_reply": "2024-10-03T20:47:04.877733Z",
     "shell.execute_reply.started": "2024-10-03T20:47:04.872666Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare messages for the API in JSON format\n",
    "messages = [{\"role\": \"system\", \"content\": (\"You are a friendly chatbot who always has a positive attitude. You are here to help customers with their questions. Please keep your answer brief and relevant. If the text is not applicable to the question, reply with 'Not applicable.' You need to provide an answer based on the following content: \" + z)}, {\"role\": \"user\", \"content\": query_text}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:47:05.614234Z",
     "iopub.status.busy": "2024-10-03T20:47:05.613780Z",
     "iopub.status.idle": "2024-10-03T20:47:05.622278Z",
     "shell.execute_reply": "2024-10-03T20:47:05.620806Z",
     "shell.execute_reply.started": "2024-10-03T20:47:05.614188Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': \"You are a friendly chatbot who always has a positive attitude. You are here to help customers with their questions. Please keep your answer brief and relevant. If the text is not applicable to the question, reply with 'Not applicable.' You need to provide an answer based on the following content: Neural networks, a subset of machine learning, are computational models inspired by the human brain's neural structure. They consist of layers of nodes (neurons) connected by weighted edges, where each node processes input data through an activation function and passes it to the next layer. These networks are primarily used for tasks like classification, regression, and pattern recognition.These networks are primarily used for tasks like classification, regression, and pattern recognition. The most basic form, the feedforward neural network, processes data in one direction from input to output layers. More advanced architectures include convolutional neural networks (CNNs), which excel at image processing by learning spatial hierarchies, and recurrent neural networks (RNNs), designed for sequence data such as time series or natural language.\"},\n",
       " {'role': 'user', 'content': 'What are neural networks primarily used for?'}]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:47:06.352736Z",
     "iopub.status.busy": "2024-10-03T20:47:06.352127Z",
     "iopub.status.idle": "2024-10-03T20:47:06.359226Z",
     "shell.execute_reply": "2024-10-03T20:47:06.357910Z",
     "shell.execute_reply.started": "2024-10-03T20:47:06.352559Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "temp = \" \\n Here is my question to you: \" + query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:47:07.037075Z",
     "iopub.status.busy": "2024-10-03T20:47:07.036558Z",
     "iopub.status.idle": "2024-10-03T20:47:07.045992Z",
     "shell.execute_reply": "2024-10-03T20:47:07.044180Z",
     "shell.execute_reply.started": "2024-10-03T20:47:07.037016Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tester = \"You are a friendly chatbot who always has a positive attitude. You are here to help customers with their questions. Please keep your answer brief and relevant. If the text is not applicable to the question, reply with 'Not applicable.' You need to provide an answer based on the following content: \" + z + temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:47:07.593264Z",
     "iopub.status.busy": "2024-10-03T20:47:07.592813Z",
     "iopub.status.idle": "2024-10-03T20:47:07.692996Z",
     "shell.execute_reply": "2024-10-03T20:47:07.691844Z",
     "shell.execute_reply.started": "2024-10-03T20:47:07.593222Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "API_URL = \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\"inputs\": tester})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T20:47:08.178551Z",
     "iopub.status.busy": "2024-10-03T20:47:08.178089Z",
     "iopub.status.idle": "2024-10-03T20:47:08.186838Z",
     "shell.execute_reply": "2024-10-03T20:47:08.185401Z",
     "shell.execute_reply.started": "2024-10-03T20:47:08.178502Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNeural networks are primarily used for tasks like classification, regression, and pattern recognition.'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]['generated_text'][len(tester):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30775,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
